
<!doctype html>

<html>
<head>
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, user-scalable=yes">
  <meta name="theme-color" content="#4F7DC9">
  <meta charset="UTF-8">
  <title>Dynatrace K8s Cloud Native Operator Install &amp; Walk Through</title>
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Source+Code+Pro:400|Roboto:400,300,400italic,500,700|Roboto+Mono">
  <link rel="stylesheet" href="//fonts.googleapis.com/icon?family=Material+Icons">
  <link rel="stylesheet" href="https://storage.googleapis.com/codelab-elements-tmp/codelab-elements.css">
  <style>
    .success {
      color: #1e8e3e;
    }
    .error {
      color: red;
    }
  </style>
</head>
<body>
  <google-codelab-analytics gaid="UA-49880327-14"></google-codelab-analytics>
  <google-codelab codelab-gaid=""
                  id="dt-cloudnative-workshop"
                  title="Dynatrace K8s Cloud Native Operator Install &amp; Walk Through"
                  environment="web"
                  feedback-link="">
    
      <google-codelab-step label="Overview" duration="5">
        <h2 is-upgraded>What You&#39;ll Learn Today</h2>
<p>Today we will cover the installation and value provided by the <a href="https://github.com/Dynatrace/dynatrace-operator" target="_blank">Dynatrace Kubernetes Operator deployment</a></p>
<p>The Dynatrace operator is the k8s native way to gain full stack observability in your kubernetes and open shift environments. The Dynatrace operator provides continuous, automatic discovery &amp; observability across your kubernetes workloads. In this lab we&#39;ll work specifically with the cloud native deployment mode which will allow us to instrument only targeted namepsaces</p>
<p class="image-container"><img alt="ENVISION THE FUTURE!" src="img/6346b14ce9438bb8.png"></p>
<h3 is-upgraded>In today&#39;s lab we will</h3>
<ol type="1">
<li>Deploy the Dynatrace Operator in cloud native mode</li>
<li>Deploy a Microservice application to multiple namespaces</li>
<li>Instrumnent only the &#34;production&#34; namespace</li>
<li>Setup Persistent Volume Claim Monitoring</li>
<li>Review the full stack data collected by Dynatrace</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Technical Specification &amp; Prerequsities" duration="5">
        <h2 is-upgraded>Technologies We Will Work With Today</h2>
<ul>
<li><a href="https://www.dynatrace.com/trial/" target="_blank">Dynatrace SasS Tenant</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/" target="_blank">kubectl cli</a></li>
<li><a href="https://kubernetes.io/" target="_blank">Kubernetes</a></li>
<li><a href="https://github.com/kyledharrington/dt-boutique" target="_blank">Customized Google Microservices Demo Application</a></li>
<li><a href="https://github.com/Dynatrace/dynatrace-operator" target="_blank">Dynatrace Operator</a></li>
</ul>
<h2 is-upgraded>Prerequisities</h2>
<p>The following are assumed to be already running and/or configured for this workshop</p>
<ol type="1">
<li><a href="https://kubernetes.io/" target="_blank">A GKE based Kubernetes Cluster which you can deploy to</a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/" target="_blank">kubectl cli</a></li>
<li><a href="https://www.dynatrace.com/trial/" target="_blank">A Dynatrace SasS Tenant</a></li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="LAB: Perpare the dynakube yaml file" duration="5">
        <p>First we&#39;ll need to grab the configuration files deploy the Dynatrace OneAgent.</p>
<ol type="1">
<li>Navigate to: <em>DYNATRACE HUB –&gt; ONEAGENT –&gt; DOWNLOAD ONEAGENT</em><img alt="step10" src="img/e951b96f3545239d.png"></li>
<li>Select &#34;<em>Kubernetes&#34;</em><img alt="step11" src="img/89a63e9f35a63091.png"></li>
<li>At the next screen we&#39;ll need to provide some values:</li>
<li>Populate the name field, note that this must be unique to your environment</li>
<li>Click on &#34;create tokens&#34;, this will generate new tokens for the operator deployment</li>
<li>Toggle on &#34;skip ssl cert check&#34;</li>
<li>Toggle on &#34;Enable Volume Storage&#34;</li>
<li>Click on &#34;Download dynakube.yaml&#34; <img alt="step12" src="img/6e3a7aa24e02614b.png"></li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="LAB: Deploy the Dynatrace operator" duration="5">
        <p>Next we&#39;ll create a namespace for all the Dynatrace components. Then deploy the Operator and the CSI <a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/" target="_blank">(Container Storage Interface)</a> pods.</p>
<ol type="1">
<li>Create the dynatrace namespace:  <pre><code>kubectl create namespace dynatrace
</code></pre>
</li>
<li>Deploy the Dynatrace operator  <pre><code>kubectl apply -f https://github.com/Dynatrace/dynatrace-operator/releases/latest/download/kubernetes.yaml
</code></pre>
</li>
<li>Verify that the Dynatrace operator and webhook pods have started:  <pre><code>kubectl get pod -n dynatrace
</code></pre>
</li>
<li>Your output should be similar  <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE
dynatrace-operator-678d8f9d75-tmnn2   1/1     Running   0          54s
dynatrace-webhook-b798d4cb5-nh426     1/1     Running   0          54s
dynatrace-webhook-b798d4cb5-tzz5q     1/1     Running   0          54s
</code></pre>
</li>
<li>deploy the dynatrace csi drivers. Please note these are required for CloudNative mode  <pre><code>kubectl apply -f https://github.com/Dynatrace/dynatrace-operator/releases/latest/download/kubernetes-csi.yaml
</code></pre>
</li>
<li>This will deploy a Daemonset to your cluster  <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE
dynatrace-oneagent-csi-driver-d4rr6   3/3     Running   0          5m33s
dynatrace-oneagent-csi-driver-hbpdh   3/3     Running   0          5m33s
dynatrace-oneagent-csi-driver-pj2xm   3/3     Running   0          5m33s
dynatrace-operator-678d8f9d75-tmnn2   1/1     Running   0          9m52s
dynatrace-webhook-b798d4cb5-nh426     1/1     Running   0          9m52s
dynatrace-webhook-b798d4cb5-tzz5q     1/1     Running   0          9m52s
</code></pre>
</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="LAB: Configure and Deploy OneAgent for Cloud Native Mode" duration="15">
        <p>Now we can deploy an application to our kubernetes cluster. This does require two changes to the dynakube yaml file we downloaded from our tenant.</p>
<ol type="1">
<li>Add the below snippet to the dynakube.yaml just above the oneAgent section at approxmiately line 60  <pre><code>namespaceSelector:
  matchLabels:
    could-native: enabled
</code></pre>
</li>
<li>Change &#34;the classicFullStack&#34; value under the oneAgent stanza to &#34;cloudNativeFullStack&#34;</li>
<li>Before  <pre><code>oneAgent:
# Optional: enable classic fullstack monitoring and change its settings
# Cannot be used in conjunction with cloud-native fullstack monitoring, application-only monitoring or host monitoring
  classicFullStack:
    # Optional: If specified, indicates the OneAgent version to use
    # Defaults to latest
    #
    # version:
</code></pre>
</li>
<li>After  <pre><code>oneAgent:
# Optional: enable classic fullstack monitoring and change its settings
# Cannot be used in conjunction with cloud-native fullstack monitoring, application-only monitoring or host monitoring
  cloudNativeFullStack:
    # Optional: If specified, indicates the OneAgent version to use
    # Defaults to latest
    #
    # version:
</code></pre>
</li>
<li>The two modifications made should be next to each other, with you final dynakube.yaml looking like this  <pre><code># Optional: when enabled, and if Istio is installed on the Kubernetes environment, then the
# Operator will create the corresponding VirtualService and ServiceEntry objects to allow access
# to the Dynatrace cluster from the agent or the activeGate. Disabled by default.
#
# enableIstio: false
namespaceSelector:
  matchLabels:
    could-native: enabled
oneAgent:
  # Optional: enable classic fullstack monitoring and change its settings
  # Cannot be used in conjunction with cloud-native fullstack monitoring, application-only monitoring or host monitoring
  cloudNativeFullStack:
    # Optional: If specified, indicates the OneAgent version to use
    # Defaults to latest
    #
    # version:
</code></pre>
</li>
<li>Make sure all changed have been saved and deploy the Dynatrace operator  <pre><code>kubectl apply -f dynakube.yaml
</code></pre>
</li>
<li>The below output will print when the operator has finished installing  <pre><code>secret/dynatrace-lab created
dynakube.dynatrace.com/dynatrace-lab created
</code></pre>
</li>
<li>Verify that all pods have been succefully deployed by running  <pre><code>kubectl get pods -n dynatrace
</code></pre>
</li>
<li>Which should out put something similar to below:  <pre><code>NAME                                  READY   STATUS    RESTARTS   AGE
dynatrace-lab-activegate-0            1/1     Running   0          2m41s
dynatrace-lab-oneagent-5mhf9          1/1     Running   0          2m42s
dynatrace-lab-oneagent-6vqcd          1/1     Running   0          2m42s
dynatrace-lab-oneagent-t4bz7          1/1     Running   0          2m42s
dynatrace-oneagent-csi-driver-d4rr6   3/3     Running   0          29m
dynatrace-oneagent-csi-driver-hbpdh   3/3     Running   0          29m
dynatrace-oneagent-csi-driver-pj2xm   3/3     Running   0          29m
dynatrace-operator-678d8f9d75-tmnn2   1/1     Running   0          33m
dynatrace-webhook-b798d4cb5-nh426     1/1     Running   0          33m
dynatrace-webhook-b798d4cb5-tzz5q     1/1     Running   0          33m
</code></pre>
</li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="LAB: Kubernetes API &amp; PVC Itegration" duration="15">
        <p>Now that we&#39;ve gotten our pods up and running we&#39;ll turn on feature flags the Kubernetes API and enable PVC metric ingestion in our cluster. These features are both dependent on the activegate we deployed with the Dynakube yaml in the last step.</p>
<ol type="1">
<li>In your Dynatrace tenant, navigate to <em>INFRASTRUCTURE –&gt; KUBERNETES</em> Here you can your newly deployed cluster and any other k8s cluster you may have monitored with Dynatrace. <img alt="step14" src="img/582c9faaa54cb283.png"></li>
<li>Click on &#34;...&#34; under &#34;actions&#34; and select &#34;Settings&#34; <img alt="step14" src="img/fa68a1a59c3c3516.png"></li>
<li>Enable all available feature flags</li>
<li>Click on &#34;Save Changes&#34; <img alt="step14" src="img/d156cdae48fe1db6.png"></li>
<li>In your Dynatrace tenant, navigate to <em>DYNATRACE HUB –&gt; Search for &#34;PVC&#34; –&gt;</em><img alt="step14" src="img/17ff871fe0560fe8.png"></li>
<li>Click on &#34;Add to environment&#34; <img alt="step14" src="img/729b73d34dece5c6.png"></li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="LAB: The Kubernetes API Integration" duration="10">
        <h2 is-upgraded>Lets review the data Dynatrace has collected already</h2>
<p>Out of the box, Dynatrace automatically collects telemetry for</p>
<ul>
<li>Cluster utilization metrics  <ul>
<li>CPU &amp; Memory Usage, Request and Limits</li>
</ul>
</li>
<li>Kubernetes Workload Metrics Aggregated by  <ul>
<li>Workload Type</li>
<li>Pods</li>
<li>Namespaces</li>
</ul>
</li>
<li>Kubernetes Vulnerabilities (<code>note: this module may not be enabled in your environment</code>)</li>
</ul>
<p class="image-container"><img alt="step14" src="img/623be72ec45353d4.png"></p>
<p>Lets also note what is missing, <em>Kubernetes events!</em></p>
<p>We configured Dynatrace to capture all events in this cluster in our last step (enabling &#34;monitor events&#34;) We will now create a new application deployment which will populate Kubernetes native events in our dashboard</p>


      </google-codelab-step>
    
      <google-codelab-step label="LAB: Deploying the DT-Boutique Micro Services Application" duration="10">
        <p>Now we will deploy our sample application across two namespaces. We will use the Google Microservices demo application &#34;Online boutique&#34;, this version has been branched from the <a href="https://github.com/GoogleCloudPlatform/microservices-demo" target="_blank">original</a>. This version uses and nginx ingresses and pvc for the redis pod for the purposes of this lab.</p>
<ol type="1">
<li>First we will create two namespaces, one for a lower envrionment and one for production.  <pre><code>kubectl create namespace lower
kubectl create namespace production
</code></pre>
</li>
<li>Next we will label the production namespace for full stack injection  <pre><code>kubectl label namespace production cloud-native=enabled
</code></pre>
</li>
<li>With this label in place only the production namespace, and any other namspace with this label applied, will be instrumented via fullstack when we deploy. All others will  This specific key=value pair was set in step 5 of the lab by setting the <code>namespaceSelector:</code> value. This can be modified to match your own environment variables.</li>
<li>Lets deploy the dt-boutique application to both the lower and production namespaces  <pre><code>kubectl apply -f https://raw.githubusercontent.com/kyledharrington/dt-boutique/main/dt-boutique.yaml -n lower
kubectl apply -f https://raw.githubusercontent.com/kyledharrington/dt-boutique/main/dt-boutique.yaml -n production 
</code></pre>
</li>
<li>It should take about 2 minutes for all pods come up in both namespaces. You can check pods status by running:  <pre><code>kubectl get pod -n production; kubectl get pod -n lower
</code></pre>
</li>
<li>Once all your pods are running you can find the external endpoint to access the application by running:  <pre><code>kubectl get svc --all-namespaces| grep nginx
</code></pre>
<pre><code>lower         nginx                      LoadBalancer   10.28.0.96     34.82.229.185   80:32116/TCP     4d7h
production    nginx                      LoadBalancer   10.28.14.73    34.105.55.209   80:32543/TCP     4d7h
</code></pre>
</li>
<li>Copy the EXTERNAL-IP address and paste it into a browser: <img alt="step14" src="img/6d24dd30f08158a8.png"></li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="LAB: Exploring The Telemetry Collected by Dynatrace" duration="15">
        <p>While we configured the front end of the application, the OneAgent daemonset we deployed earlier has been collecting our newly deployed application&#39;s telemetry data.</p>
<h3 is-upgraded>Kubernetes Specific Data points</h3>
<ol type="1">
<li>In your Dynatrace tenant, navigate to <em>INFRASTRUCTURE –&gt; KUBERNETES</em></li>
<li>If we scroll down to the events section, we&#39;ll see that this now populating k8s native events from the google microservices deployment:  <ul>
<li>Image pulls</li>
<li>New scheduling events</li>
<li>Pod starts <img alt="step14" src="img/6050ca7e1bb18bd3.png"></li>
</ul>
</li>
</ol>
<h3 is-upgraded>Persistent Volume Claim Data Points</h3>
<ol type="1">
<li>The configured PVC entention enables multiple new PVC specific metrics from the kubelet volumes for customized reporting and dashboarding. <img alt="step14" src="img/8c9db719ada584d1.png"></li>
<li>These metric data points are also automatically populated for PVC statistics in an out of the box dashboard, &#34;Kubernetes Persistent Volume Claims&#34; for your clusters once enabled: <img alt="step14" src="img/201dff579b5a5350.png"></li>
</ol>
<h3 is-upgraded>Cloud Native Full stack differences:</h3>
<ol type="1">
<li>In the lower environmnet where we did not configure application injection, while we do not provide &#34;service&#34; or application performance metrics, we are still collecting all infrastrucute and pod level data points:</li>
</ol>
<p class="image-container"><img alt="step14" src="img/1269961cffdb827e.png"></p>
<ol type="1">
<li>Meanwhile in production Dynatrace is providing full-stack visibility:  <img alt="step14" src="img/2f2049211fbf115f.png"></li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="LAB: Kubernetes Scaling with Dynatrace" duration="15">
        <p>Now that we&#39;ve seen how to dynatrace instruments an existing application lets see how dynatrace handles and instruments pods scaling in our production environment</p>
<ol type="1">
<li>Back in the cloud console lets see how many pods are currently running in the production namespace  <pre><code>kubectl get pod -n production
</code></pre>
</li>
<li>Your console should return similar to the below: <img alt="step14" src="img/95376c1add084051.png">  <pre><code>kubectl get deployments -n production
</code></pre>
</li>
<li>Lets take a look a the deployments in the namespace, specifically the cart service <img alt="step14" src="img/ee356c75018cdf15.png"></li>
<li>Lets scale the cart service up to 10 pods  <pre><code>kubectl scale deployment -n production cartservice --replicas=10
</code></pre>
</li>
<li>This will begin spinning up new pods. You can see this by running:  <pre><code>kubectl get pods -n production
</code></pre>
</li>
<li>Dynatrace will automatically instrument these new pods with code level visibility. If we were to do this in the &#34;lower&#34; namespace we would only have infrastructure level visibility.</li>
<li>Once the 10 new pods have stabilized, let see how Dynatrace handles a pod failure event. Lets scale these pods up simulate either a DDOS attack triggering an HPA or an under provisioned Kubernetes cluster.</li>
</ol>
<h3 is-upgraded>Simulating a Reliability Engineering scenario with Dynatrace</h3>
<ol type="1">
<li>Lets scale the cart service up to 100 pods &gt;:)  <pre><code>kubectl scale deployment -n production cartservice --replicas=100
</code></pre>
<img alt="step14" src="img/a58e23dac8709e9a.png"></li>
<li>Navigate to <em>Kubernetes Engine –&gt; Clusters –&gt; Workloads   </em><img alt="step14" src="img/5580a2082d2ea37f.png"></li>
<li>From here we can immediately see that dynatrace has found an issue with trying to scale up the 100 pods we requested:  <ul>
<li>all pods have been grouped together under the cart service work load</li>
<li>only 48 of the 100 we we requested we scheduleable</li>
<li>Dynatrace has automatically created a problem beacuase of the resource contention <img alt="step14" src="img/129c37a956f5c410.png"></li>
</ul>
</li>
<li>With this information, Dynatrace can be leveraged to proactively scale these nodes or descrease the pods scaling to resolve the resouce contention. <img alt="step14" src="img/17e819bd7d9da516.png"></li>
<li>Lets scale the pods back down  <pre><code>kubectl scale deployment cartservice --replicas=10
</code></pre>
</li>
<li>From the workload level we can see the scaling events taking place:  <img alt="step14" src="img/155c26dba0e1cd5b.png"></li>
<li>Once the pods stabilize dynatrace will automatically close this problem: <img alt="step14" src="img/c28cf3d9bb235fe1.png"></li>
</ol>


      </google-codelab-step>
    
      <google-codelab-step label="Lab: Wrap Up" duration="5">
        <h2 is-upgraded>Lab recap</h2>
<p>Today we reviewed how the dynatrace platform can be leveraged to provide full stack observability across your kubernetes environments. To you learned how to</p>
<ol type="1">
<li>Deploy the Dynatrace Operator in cloud native mode</li>
<li>Deploy a Microservice application to multiple namespaces</li>
<li>Instrumnent only the &#34;production&#34; namespace with Dynatrace</li>
<li>Setup Persistent Volume Claim Monitoring</li>
<li>Review all data collected by Dynatrace</li>
</ol>


      </google-codelab-step>
    
  </google-codelab>

  <script src="https://storage.googleapis.com/codelab-elements-tmp/native-shim.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements-tmp/custom-elements.min.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements-tmp/prettify.js"></script>
  <script src="https://storage.googleapis.com/codelab-elements-tmp/codelab-elements.js"></script>
  <script src="//support.google.com/inapp/api.js"></script>

</body>
</html>
